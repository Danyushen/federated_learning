{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from monai.utils import first, set_determinism\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    AsDiscreted,\n",
    "    EnsureChannelFirstd,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    SaveImaged,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    "    Invertd,\n",
    ")\n",
    "from monai.handlers.utils import from_engine\n",
    "from monai.networks.nets import UNet\n",
    "from monai.networks.layers import Norm\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.losses import DiceLoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.data import CacheDataset, DataLoader, Dataset, decollate_batch\n",
    "from monai.config import print_config\n",
    "from monai.apps import download_and_extract\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "import shutil\n",
    "import os\n",
    "import glob\n",
    "import tarfile\n",
    "import numpy as np\n",
    "from torch.utils.data import Subset\n",
    "import copy\n",
    "set_determinism(seed=0)\n",
    "np.random.seed(0)\n",
    "\n",
    "#print_config()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spleen_dataset(data_dir):\n",
    "    train_images = sorted(glob.glob(os.path.join(data_dir, \"imagesTr\", \"*.nii.gz\")))\n",
    "    print(train_images)\n",
    "    train_labels = sorted(glob.glob(os.path.join(data_dir, \"labelsTr\", \"*.nii.gz\")))\n",
    "    data_dicts = [{\"image\": image_name, \"label\": label_name} for image_name, label_name in zip(train_images, train_labels)]\n",
    "    train_files, val_files = data_dicts[:-9], data_dicts[-9:]\n",
    "    \n",
    "    train_transforms = Compose(\n",
    "        [\n",
    "            LoadImaged(keys=[\"image\", \"label\"]),\n",
    "            EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "            ScaleIntensityRanged(\n",
    "                keys=[\"image\"],\n",
    "                a_min=-57,\n",
    "                a_max=164,\n",
    "                b_min=0.0,\n",
    "                b_max=1.0,\n",
    "                clip=True,\n",
    "            ),\n",
    "            CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
    "            Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "            Spacingd(keys=[\"image\", \"label\"], pixdim=(1.5, 1.5, 2.0), mode=(\"bilinear\", \"nearest\")),\n",
    "            RandCropByPosNegLabeld(\n",
    "                keys=[\"image\", \"label\"],\n",
    "                label_key=\"label\",\n",
    "                spatial_size=(96, 96, 96),\n",
    "                pos=1,\n",
    "                neg=1,\n",
    "                num_samples=4,\n",
    "                image_key=\"image\",\n",
    "                image_threshold=0,\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    val_transforms = Compose(\n",
    "        [\n",
    "            LoadImaged(keys=[\"image\", \"label\"]),\n",
    "            EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "            ScaleIntensityRanged(\n",
    "                keys=[\"image\"],\n",
    "                a_min=-57,\n",
    "                a_max=164,\n",
    "                b_min=0.0,\n",
    "                b_max=1.0,\n",
    "                clip=True,\n",
    "            ),\n",
    "            CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
    "            Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "            Spacingd(keys=[\"image\", \"label\"], pixdim=(1.5, 1.5, 2.0), mode=(\"bilinear\", \"nearest\")),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    train_ds = CacheDataset(data=train_files, transform=train_transforms, cache_rate=1.0, num_workers=4)\n",
    "    val_ds = CacheDataset(data=val_files, transform=val_transforms, cache_rate=1.0, num_workers=4)\n",
    "    \n",
    "    return train_ds, val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset_iid(train_dataset, val_dataset, num_clients):\n",
    "    #make the dataset evenly divisible by removing the last elements if needed\n",
    "    train_dataset = train_dataset[:len(train_dataset) - len(train_dataset) % num_clients]\n",
    "    val_dataset = val_dataset[:len(val_dataset) - len(val_dataset) % num_clients]\n",
    "    \n",
    "    #split the dataset into num_clients subsets randomly \n",
    "    train_indices = np.random.permutation(len(train_dataset))\n",
    "    val_indices = np.random.permutation(len(val_dataset))\n",
    "    train_client_splits = np.array_split(train_indices, num_clients)\n",
    "    val_client_splits = np.array_split(val_indices, num_clients)\n",
    "    \n",
    "    #create a list of subsets\n",
    "    train_client_datasets = [Subset(train_dataset, split) for split in train_client_splits]\n",
    "    val_client_datasets = [Subset(val_dataset, split) for split in val_client_splits]\n",
    "    \n",
    "    return train_client_datasets, val_client_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset_non_iid(train_dataset, val_dataset, num_clients):\n",
    "    if len(train_dataset) % num_clients != 0 or len(val_dataset) % num_clients != 0:\n",
    "         #split the dataset into num_clients subsets randomly \n",
    "        train_indices = np.random.permutation(len(train_dataset))\n",
    "        val_indices = np.random.permutation(len(val_dataset))\n",
    "        train_client_splits = np.array_split(train_indices, num_clients)\n",
    "        val_client_splits = np.array_split(val_indices, num_clients)\n",
    "        \n",
    "        #create a list of subsets\n",
    "        train_client_datasets = [Subset(train_dataset, split) for split in train_client_splits]\n",
    "        val_client_datasets = [Subset(val_dataset, split) for split in val_client_splits]\n",
    "        \n",
    "        return train_client_datasets, val_client_datasets\n",
    "    \n",
    "    else: \n",
    "        while len(train_dataset) % num_clients == 0:\n",
    "            train_dataset = train_dataset[:len(train_dataset) - 1]\n",
    "        return split_dataset_non_iid(train_dataset, val_dataset, num_clients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the data path\n",
    "root_dir = os.path.dirname(os.path.dirname(os.path.abspath(\"federated_learning\")))\n",
    "data_dir = os.path.join(root_dir, \"data/raw/Task09_Spleen/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/danyu/Desktop/SJTU/Distributed_ML_Systems/federated_learning/data/raw/Task09_Spleen/imagesTr/spleen_10.nii.gz', '/Users/danyu/Desktop/SJTU/Distributed_ML_Systems/federated_learning/data/raw/Task09_Spleen/imagesTr/spleen_12.nii.gz', '/Users/danyu/Desktop/SJTU/Distributed_ML_Systems/federated_learning/data/raw/Task09_Spleen/imagesTr/spleen_13.nii.gz', '/Users/danyu/Desktop/SJTU/Distributed_ML_Systems/federated_learning/data/raw/Task09_Spleen/imagesTr/spleen_14.nii.gz', '/Users/danyu/Desktop/SJTU/Distributed_ML_Systems/federated_learning/data/raw/Task09_Spleen/imagesTr/spleen_16.nii.gz', '/Users/danyu/Desktop/SJTU/Distributed_ML_Systems/federated_learning/data/raw/Task09_Spleen/imagesTr/spleen_17.nii.gz', '/Users/danyu/Desktop/SJTU/Distributed_ML_Systems/federated_learning/data/raw/Task09_Spleen/imagesTr/spleen_18.nii.gz', '/Users/danyu/Desktop/SJTU/Distributed_ML_Systems/federated_learning/data/raw/Task09_Spleen/imagesTr/spleen_19.nii.gz', '/Users/danyu/Desktop/SJTU/Distributed_ML_Systems/federated_learning/data/raw/Task09_Spleen/imagesTr/spleen_2.nii.gz', '/Users/danyu/Desktop/SJTU/Distributed_ML_Systems/federated_learning/data/raw/Task09_Spleen/imagesTr/spleen_20.nii.gz', '/Users/danyu/Desktop/SJTU/Distributed_ML_Systems/federated_learning/data/raw/Task09_Spleen/imagesTr/spleen_21.nii.gz', '/Users/danyu/Desktop/SJTU/Distributed_ML_Systems/federated_learning/data/raw/Task09_Spleen/imagesTr/spleen_22.nii.gz', '/Users/danyu/Desktop/SJTU/Distributed_ML_Systems/federated_learning/data/raw/Task09_Spleen/imagesTr/spleen_24.nii.gz', '/Users/danyu/Desktop/SJTU/Distributed_ML_Systems/federated_learning/data/raw/Task09_Spleen/imagesTr/spleen_25.nii.gz', '/Users/danyu/Desktop/SJTU/Distributed_ML_Systems/federated_learning/data/raw/Task09_Spleen/imagesTr/spleen_26.nii.gz', '/Users/danyu/Desktop/SJTU/Distributed_ML_Systems/federated_learning/data/raw/Task09_Spleen/imagesTr/spleen_27.nii.gz', '/Users/danyu/Desktop/SJTU/Distributed_ML_Systems/federated_learning/data/raw/Task09_Spleen/imagesTr/spleen_28.nii.gz', '/Users/danyu/Desktop/SJTU/Distributed_ML_Systems/federated_learning/data/raw/Task09_Spleen/imagesTr/spleen_29.nii.gz', '/Users/danyu/Desktop/SJTU/Distributed_ML_Systems/federated_learning/data/raw/Task09_Spleen/imagesTr/spleen_3.nii.gz', '/Users/danyu/Desktop/SJTU/Distributed_ML_Systems/federated_learning/data/raw/Task09_Spleen/imagesTr/spleen_31.nii.gz', '/Users/danyu/Desktop/SJTU/Distributed_ML_Systems/federated_learning/data/raw/Task09_Spleen/imagesTr/spleen_32.nii.gz', '/Users/danyu/Desktop/SJTU/Distributed_ML_Systems/federated_learning/data/raw/Task09_Spleen/imagesTr/spleen_33.nii.gz', '/Users/danyu/Desktop/SJTU/Distributed_ML_Systems/federated_learning/data/raw/Task09_Spleen/imagesTr/spleen_38.nii.gz', '/Users/danyu/Desktop/SJTU/Distributed_ML_Systems/federated_learning/data/raw/Task09_Spleen/imagesTr/spleen_40.nii.gz', '/Users/danyu/Desktop/SJTU/Distributed_ML_Systems/federated_learning/data/raw/Task09_Spleen/imagesTr/spleen_41.nii.gz', '/Users/danyu/Desktop/SJTU/Distributed_ML_Systems/federated_learning/data/raw/Task09_Spleen/imagesTr/spleen_44.nii.gz', '/Users/danyu/Desktop/SJTU/Distributed_ML_Systems/federated_learning/data/raw/Task09_Spleen/imagesTr/spleen_45.nii.gz', '/Users/danyu/Desktop/SJTU/Distributed_ML_Systems/federated_learning/data/raw/Task09_Spleen/imagesTr/spleen_46.nii.gz', '/Users/danyu/Desktop/SJTU/Distributed_ML_Systems/federated_learning/data/raw/Task09_Spleen/imagesTr/spleen_47.nii.gz', '/Users/danyu/Desktop/SJTU/Distributed_ML_Systems/federated_learning/data/raw/Task09_Spleen/imagesTr/spleen_49.nii.gz', '/Users/danyu/Desktop/SJTU/Distributed_ML_Systems/federated_learning/data/raw/Task09_Spleen/imagesTr/spleen_52.nii.gz', '/Users/danyu/Desktop/SJTU/Distributed_ML_Systems/federated_learning/data/raw/Task09_Spleen/imagesTr/spleen_53.nii.gz', '/Users/danyu/Desktop/SJTU/Distributed_ML_Systems/federated_learning/data/raw/Task09_Spleen/imagesTr/spleen_56.nii.gz', '/Users/danyu/Desktop/SJTU/Distributed_ML_Systems/federated_learning/data/raw/Task09_Spleen/imagesTr/spleen_59.nii.gz', '/Users/danyu/Desktop/SJTU/Distributed_ML_Systems/federated_learning/data/raw/Task09_Spleen/imagesTr/spleen_6.nii.gz', '/Users/danyu/Desktop/SJTU/Distributed_ML_Systems/federated_learning/data/raw/Task09_Spleen/imagesTr/spleen_60.nii.gz', '/Users/danyu/Desktop/SJTU/Distributed_ML_Systems/federated_learning/data/raw/Task09_Spleen/imagesTr/spleen_61.nii.gz', '/Users/danyu/Desktop/SJTU/Distributed_ML_Systems/federated_learning/data/raw/Task09_Spleen/imagesTr/spleen_62.nii.gz', '/Users/danyu/Desktop/SJTU/Distributed_ML_Systems/federated_learning/data/raw/Task09_Spleen/imagesTr/spleen_63.nii.gz', '/Users/danyu/Desktop/SJTU/Distributed_ML_Systems/federated_learning/data/raw/Task09_Spleen/imagesTr/spleen_8.nii.gz', '/Users/danyu/Desktop/SJTU/Distributed_ML_Systems/federated_learning/data/raw/Task09_Spleen/imagesTr/spleen_9.nii.gz']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/monai/utils/deprecate_utils.py:321: FutureWarning: monai.transforms.croppad.dictionary CropForegroundd.__init__:allow_smaller: Current default value of argument `allow_smaller=True` has been deprecated since version 1.2. It will be changed to `allow_smaller=False` in version 1.5.\n",
      "  warn_deprecated(argname, msg, warning_category)\n",
      "Loading dataset: 100%|██████████| 32/32 [00:42<00:00,  1.33s/it]\n",
      "Loading dataset: 100%|██████████| 9/9 [00:13<00:00,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 1 has 2 samples.\n",
      "Client 2 has 2 samples.\n",
      "Client 3 has 2 samples.\n",
      "Client 4 has 2 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#load the entire dataset\n",
    "train_ds, val_ds = get_spleen_dataset(data_dir)\n",
    "train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_ds, batch_size=1, num_workers=4)\n",
    "\n",
    "#split the dataset into num_clients IID subsets\n",
    "num_clients = 4\n",
    "train_client_datasets, val_client_datasets = split_dataset_iid(train_ds, val_ds, num_clients)\n",
    "\n",
    "#check the number of samples in each client\n",
    "for i, client_data in enumerate(val_client_datasets):\n",
    "    print(f\"Client {i+1} has {len(client_data)} samples.\")      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FedAvg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FedAvg:\n",
    "    def __init__(self, model, loss_function, device, num_clients, client_fraction, local_epochs, batch_size, lr=1e-4):\n",
    "        self.global_model = model\n",
    "        self.loss_function = loss_function\n",
    "        self.device = device\n",
    "        self.num_clients = num_clients\n",
    "        self.client_fraction = client_fraction\n",
    "        self.local_epochs = local_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.clients = []  # Placeholder for client datasets\n",
    "        \n",
    "        self.round_loss_values = []  # To store round-wise training losses\n",
    "        self.dice_metric_values = []  # To store round-wise Dice scores\n",
    "\n",
    "    def set_clients(self, train_datasets, val_datasets):\n",
    "        #create a list of dictionaries with train, val datasets for each client\n",
    "        self.clients = [{\"train\": train, \"val\": val} for train, val in zip(train_datasets, val_datasets)]\n",
    "\n",
    "    def client_update(self, client_data):\n",
    "        \"\"\"\n",
    "        Perform local training on a client.\n",
    "        Args:\n",
    "            client_data: A dictionary with 'train' and 'val' datasets for the client.\n",
    "        Returns:\n",
    "            Updated weights (state_dict) of the local model.\n",
    "        \"\"\"\n",
    "        #initialize a local model\n",
    "        local_model = copy.deepcopy(self.global_model)\n",
    "        local_model.train()\n",
    "        local_model.to(self.device)\n",
    "\n",
    "        #create data loader for the client's training dataset\n",
    "        train_loader = DataLoader(client_data[\"train\"], batch_size=2, shuffle=True, num_workers=4)\n",
    "        # Optimizer for the local training\n",
    "        optimizer = torch.optim.Adam(local_model.parameters(), lr=self.lr)\n",
    "        \n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        # Perform local training\n",
    "        for _ in range(self.local_epochs):\n",
    "            for batch_data in train_loader:\n",
    "                inputs, labels = (\n",
    "                    batch_data[\"image\"].to(self.device),\n",
    "                    batch_data[\"label\"].to(self.device),\n",
    "                )\n",
    "                optimizer.zero_grad()\n",
    "                outputs = local_model(inputs)\n",
    "                loss = self.loss_function(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "\n",
    "        # Calculate the average loss for this client\n",
    "        loss_per_client = total_loss / num_batches if num_batches > 0 else 0\n",
    "        # Return the updated model weights\n",
    "        return local_model.state_dict(), loss_per_client\n",
    "\n",
    "    def aggregate_updates(self, client_updates, client_sizes):\n",
    "        \"\"\"\n",
    "        Aggregate client updates to update the global model.\n",
    "        Args:\n",
    "            client_updates: List of state_dicts from clients.\n",
    "            client_sizes: List of dataset sizes for each client.\n",
    "        Returns:\n",
    "            Aggregated weights for the global model.\n",
    "        \"\"\"\n",
    "        # Initialize global weights\n",
    "        global_weights = copy.deepcopy(self.global_model.state_dict())\n",
    "        total_size = sum(client_sizes)\n",
    "\n",
    "        # Perform weighted aggregation\n",
    "        for key in global_weights.keys():\n",
    "            global_weights[key] = sum(\n",
    "                client_updates[i][key] * client_sizes[i] / total_size for i in range(len(client_updates))\n",
    "            )\n",
    "\n",
    "        return global_weights\n",
    "\n",
    "    def evaluate_global_model(self, val_datasets, post_pred, post_label, dice_metric):\n",
    "        \"\"\"\n",
    "        Evaluate the global model on all clients' validation datasets.\n",
    "        Args:\n",
    "            val_datasets: List of validation datasets for each client.\n",
    "            post_pred: Post-processing function for predictions.\n",
    "            post_label: Post-processing function for labels.\n",
    "            dice_metric: Metric instance for Dice score.\n",
    "        Returns:\n",
    "            Average Dice score across all validation datasets.\n",
    "        \"\"\"\n",
    "        self.global_model.eval()\n",
    "        dice_scores = []\n",
    "\n",
    "        for val_data in val_datasets:\n",
    "            val_loader = DataLoader(val_data, batch_size=1, num_workers=4)\n",
    "            with torch.no_grad():\n",
    "                for batch_data in val_loader:\n",
    "                    val_inputs, val_labels = (\n",
    "                        batch_data[\"image\"].to(self.device),\n",
    "                        batch_data[\"label\"].to(self.device),\n",
    "                    )\n",
    "                    roi_size = (160, 160, 160)\n",
    "                    sw_batch_size = 4\n",
    "                    val_outputs = sliding_window_inference(val_inputs, roi_size, sw_batch_size, self.global_model)\n",
    "                    val_outputs = [post_pred(i) for i in decollate_batch(val_outputs)]\n",
    "                    val_labels = [post_label(i) for i in decollate_batch(val_labels)]\n",
    "                    dice_metric(y_pred=val_outputs, y=val_labels)\n",
    "\n",
    "                # Aggregate Dice metric for this client\n",
    "                dice_score = dice_metric.aggregate().item()\n",
    "                dice_scores.append(dice_score)\n",
    "                dice_metric.reset()\n",
    "\n",
    "        # Return the average Dice score\n",
    "        return np.mean(dice_scores)\n",
    "\n",
    "    def run(self, num_rounds, post_pred, post_label, dice_metric):\n",
    "        \"\"\"\n",
    "        Run the FedAvg algorithm for the specified number of rounds.\n",
    "        Args:\n",
    "            num_rounds: Number of global training rounds.\n",
    "            post_pred: Post-processing function for predictions.\n",
    "            post_label: Post-processing function for labels.\n",
    "            dice_metric: Metric instance for Dice score.\n",
    "        \"\"\"\n",
    "        global_weights = self.global_model.state_dict()\n",
    "\n",
    "        for round_num in range(num_rounds):\n",
    "            print(f\"--- Round {round_num + 1} ---\")\n",
    "\n",
    "            # Randomly select clients\n",
    "            num_selected_clients = max(int(self.client_fraction * self.num_clients), 1)\n",
    "            selected_clients = np.random.choice(self.num_clients, num_selected_clients, replace=False)\n",
    "\n",
    "            client_updates = []\n",
    "            client_sizes = []\n",
    "            total_loss = 0\n",
    "\n",
    "            # Each selected client performs local training\n",
    "            for client_id in selected_clients:\n",
    "                client_data = self.clients[client_id]\n",
    "                client_size = len(client_data[\"train\"])\n",
    "                client_sizes.append(client_size)\n",
    "\n",
    "                updated_weights, loss_per_client = self.client_update(client_data)\n",
    "                client_updates.append(updated_weights)\n",
    "                total_loss += loss_per_client\n",
    "\n",
    "            # Aggregate updates to update global model weights\n",
    "            global_weights = self.aggregate_updates(client_updates, client_sizes)\n",
    "            self.global_model.load_state_dict(global_weights)\n",
    "\n",
    "            # Evaluate the global model on all validation datasets\n",
    "            avg_dice_score = self.evaluate_global_model(\n",
    "                [client[\"val\"] for client in self.clients],\n",
    "                post_pred,\n",
    "                post_label,\n",
    "                dice_metric,\n",
    "            )\n",
    "            print(f\"Average Dice Score after Round {round_num + 1}: {avg_dice_score:.4f}\")\n",
    "\n",
    "            self.dice_metric_values.append(avg_dice_score)\n",
    "            avg_loss_per_round = total_loss / len(selected_clients) if len(selected_clients) > 0 else 0\n",
    "            self.round_loss_values.append(avg_loss_per_round)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FedCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implement FedCluster***: https://arxiv.org/pdf/2009.10748\n",
    "class FedCluster:\n",
    "    def __init__(self, model, loss_function, device, num_clients, client_fraction, local_epochs, batch_size, lr=1e-4, num_clusters=2):\n",
    "        self.global_model = model\n",
    "        self.loss_function = loss_function\n",
    "        self.device = device\n",
    "        self.num_clients = num_clients\n",
    "        self.client_fraction = client_fraction\n",
    "        self.local_epochs = local_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.num_clusters = num_clusters\n",
    "        self.clients = []  \n",
    "        self.clusters = []  \n",
    "        \n",
    "        self.round_loss_values = []  \n",
    "        self.dice_metric_values = []  \n",
    "\n",
    "    def set_clients(self, train_datasets, val_datasets):\n",
    "        # Create a list of dictionaries with train and val datasets for each client\n",
    "        self.clients = [{\"train\": train, \"val\": val} for train, val in zip(train_datasets, val_datasets)]\n",
    "        self.create_clusters()\n",
    "\n",
    "    def create_clusters(self):\n",
    "        # Randomly group clients into clusters\n",
    "        client_ids = np.arange(self.num_clients)\n",
    "        np.random.shuffle(client_ids)\n",
    "        cluster_size = self.num_clients // self.num_clusters\n",
    "        self.clusters = [client_ids[i:i + cluster_size] for i in range(0, len(client_ids), cluster_size)]\n",
    "\n",
    "    def client_update(self, client_data):\n",
    "        \"\"\"\n",
    "        Perform local training on a client.\n",
    "        \"\"\"\n",
    "        local_model = copy.deepcopy(self.global_model)\n",
    "        local_model.train()\n",
    "        local_model.to(self.device)\n",
    "\n",
    "        train_loader = DataLoader(client_data[\"train\"], batch_size=self.batch_size, shuffle=True, num_workers=4)\n",
    "        optimizer = torch.optim.Adam(local_model.parameters(), lr=self.lr)\n",
    "\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for _ in range(self.local_epochs):\n",
    "            for batch_data in train_loader:\n",
    "                inputs, labels = (\n",
    "                    batch_data[\"image\"].to(self.device),\n",
    "                    batch_data[\"label\"].to(self.device),\n",
    "                )\n",
    "                optimizer.zero_grad()\n",
    "                outputs = local_model(inputs)\n",
    "                loss = self.loss_function(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "        \n",
    "        loss_per_client = total_loss / num_batches if num_batches > 0 else 0\n",
    "        return local_model.state_dict(), loss_per_client\n",
    "\n",
    "    def aggregate_updates(self, client_updates, client_sizes):\n",
    "        \"\"\"\n",
    "        Aggregate client updates to update the global model.\n",
    "        \"\"\"\n",
    "        global_weights = copy.deepcopy(self.global_model.state_dict())\n",
    "        total_size = sum(client_sizes)\n",
    "\n",
    "        for key in global_weights.keys():\n",
    "            global_weights[key] = sum(\n",
    "                client_updates[i][key] * client_sizes[i] / total_size for i in range(len(client_updates))\n",
    "            )\n",
    "\n",
    "        return global_weights\n",
    "\n",
    "    def run(self, num_rounds, post_pred, post_label, dice_metric):\n",
    "        \"\"\"\n",
    "        Run the FedCluster algorithm.\n",
    "        \"\"\"\n",
    "        global_weights = self.global_model.state_dict()\n",
    "\n",
    "        for round_num in range(num_rounds):\n",
    "            print(f\"--- Round {round_num + 1} ---\")\n",
    "            for cluster_idx, cluster in enumerate(self.clusters):\n",
    "                print(f\"Processing Cluster {cluster_idx + 1}\")\n",
    "\n",
    "                # Randomly select clients from the cluster\n",
    "                num_selected_clients = max(int(self.client_fraction * len(cluster)), 1)\n",
    "                selected_clients = np.random.choice(cluster, num_selected_clients, replace=False)\n",
    "\n",
    "                client_updates = []\n",
    "                client_sizes = []\n",
    "                total_loss = 0\n",
    "\n",
    "                # Each selected client performs local training\n",
    "                for client_id in selected_clients:\n",
    "                    client_data = self.clients[client_id]\n",
    "                    client_size = len(client_data[\"train\"])\n",
    "                    client_sizes.append(client_size)\n",
    "\n",
    "                    updated_weights, loss_per_client = self.client_update(client_data)\n",
    "                    client_updates.append(updated_weights)\n",
    "                    total_loss += loss_per_client\n",
    "                    \n",
    "                # Aggregate updates from the cluster\n",
    "                global_weights = self.aggregate_updates(client_updates, client_sizes)\n",
    "                self.global_model.load_state_dict(global_weights)\n",
    "\n",
    "            # Evaluate the global model on all validation datasets\n",
    "            avg_dice_score = self.evaluate_global_model(\n",
    "                [client[\"val\"] for client in self.clients],\n",
    "                post_pred,\n",
    "                post_label,\n",
    "                dice_metric,\n",
    "            )\n",
    "            print(f\"Average Dice Score after Round {round_num + 1}: {avg_dice_score:.4f}\")\n",
    "\n",
    "            self.dice_metric_values.append(avg_dice_score)\n",
    "            avg_loss_per_round = total_loss / len(selected_clients) if len(selected_clients) > 0 else 0\n",
    "            self.round_loss_values.append(avg_loss_per_round)\n",
    "            \n",
    "    def evaluate_global_model(self, val_datasets, post_pred, post_label, dice_metric):\n",
    "        \"\"\"\n",
    "        Evaluate the global model on all clients' validation datasets.\n",
    "        \"\"\"\n",
    "        self.global_model.eval()\n",
    "        dice_scores = []\n",
    "\n",
    "        for val_data in val_datasets:\n",
    "            val_loader = DataLoader(val_data, batch_size=1, num_workers=4)\n",
    "            with torch.no_grad():\n",
    "                for batch_data in val_loader:\n",
    "                    val_inputs, val_labels = (\n",
    "                        batch_data[\"image\"].to(self.device),\n",
    "                        batch_data[\"label\"].to(self.device),\n",
    "                    )\n",
    "                    roi_size = (160, 160, 160)\n",
    "                    sw_batch_size = 4\n",
    "                    val_outputs = sliding_window_inference(val_inputs, roi_size, sw_batch_size, self.global_model)\n",
    "                    val_outputs = [post_pred(i) for i in decollate_batch(val_outputs)]\n",
    "                    val_labels = [post_label(i) for i in decollate_batch(val_labels)]\n",
    "                    dice_metric(y_pred=val_outputs, y=val_labels)\n",
    "\n",
    "                dice_score = dice_metric.aggregate().item()\n",
    "                dice_scores.append(dice_score)\n",
    "                dice_metric.reset()\n",
    "\n",
    "        return np.mean(dice_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training FedAvg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(\n",
    "    spatial_dims=3,\n",
    "    in_channels=1,\n",
    "    out_channels=2,\n",
    "    channels=(16, 32, 64, 128, 256),\n",
    "    strides=(2, 2, 2, 2),\n",
    "    num_res_units=2,\n",
    "    norm=Norm.BATCH,\n",
    ").to(device)\n",
    "loss_function = DiceLoss(to_onehot_y=True, softmax=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-4)\n",
    "dice_metric = DiceMetric(include_background=False, reduction=\"mean\")\n",
    "\n",
    "num_clients = 4  # Total number of clients\n",
    "client_fraction = 0.5  # Fraction of clients selected per round\n",
    "local_epochs = 2  # Number of epochs each client trains locally\n",
    "post_pred = Compose([AsDiscrete(argmax=True, to_onehot=2)])\n",
    "post_label = Compose([AsDiscrete(to_onehot=2)])\n",
    "\n",
    "fedavg = FedAvg(\n",
    "    model=model,\n",
    "    loss_function=loss_function,\n",
    "    device=device,\n",
    "    num_clients=num_clients,\n",
    "    client_fraction=0.5,  # Fraction of clients participating in each round\n",
    "    local_epochs=2,       # Number of local epochs per client\n",
    "    batch_size=2,         # Batch size for local training\n",
    "    lr=1e-4               # Learning rate\n",
    ")\n",
    "\n",
    "fedavg.set_clients(train_client_datasets, val_client_datasets)\n",
    "\n",
    "fedavg.run(\n",
    "    num_rounds=200,           # Number of global rounds\n",
    "    post_pred=post_pred,     # Post-processing for predictions\n",
    "    post_label=post_label,   # Post-processing for labels\n",
    "    dice_metric=dice_metric  # Dice metric instance\n",
    ")\n",
    "\n",
    "torch.save({\n",
    "    'round_losses': fedavg.round_loss_values,\n",
    "    'dice_scores': fedavg.dice_metric_values,\n",
    "    'global_model_state': fedavg.global_model.state_dict()\n",
    "}, 'fedavg_r200_iid.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training FedCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(\n",
    "    spatial_dims=3,\n",
    "    in_channels=1,\n",
    "    out_channels=2,\n",
    "    channels=(16, 32, 64, 128, 256),\n",
    "    strides=(2, 2, 2, 2),\n",
    "    num_res_units=2,\n",
    "    norm=Norm.BATCH,\n",
    ").to(device)\n",
    "loss_function = DiceLoss(to_onehot_y=True, softmax=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-4)\n",
    "dice_metric = DiceMetric(include_background=False, reduction=\"mean\")\n",
    "\n",
    "\n",
    "num_clients = 4  # Total number of clients\n",
    "client_fraction = 0.5  # Fraction of clients selected per round\n",
    "local_epochs = 2  # Number of epochs each client trains locally\n",
    "post_pred = Compose([AsDiscrete(argmax=True, to_onehot=2)])\n",
    "post_label = Compose([AsDiscrete(to_onehot=2)])\n",
    "\n",
    "fedcluster = FedCluster(\n",
    "    model=model,\n",
    "    loss_function=loss_function,\n",
    "    device=device,\n",
    "    num_clients=num_clients,\n",
    "    client_fraction=client_fraction,\n",
    "    local_epochs=local_epochs,\n",
    "    batch_size=2,\n",
    "    lr=1e-4,\n",
    "    num_clusters=2,\n",
    ")\n",
    "\n",
    "fedcluster.set_clients(train_client_datasets, val_client_datasets)\n",
    "\n",
    "num_rounds = 200\n",
    "fedcluster.run(num_rounds, post_pred, post_label, dice_metric)\n",
    "\n",
    "torch.save({\n",
    "    'round_losses': fedcluster.round_loss_values,\n",
    "    'dice_scores': fedcluster.dice_metric_values,\n",
    "    'global_model_state': fedcluster.global_model.state_dict()\n",
    "}, 'fedc_r200_iid.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
